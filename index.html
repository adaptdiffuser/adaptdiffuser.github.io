<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/html">
  <head>
    <meta charset="utf-8" />
    <meta
      name="description"
      content="AdaptDiffuser: Diffusion Models as Adaptive Self-evolving Planners."
    />
    <meta name="keywords" content="Reinforcement Learning, Diffusion Model, Trajectory Optimization" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>
      AdaptDiffuser: Diffusion Models as Adaptive Self-evolving Planners
    </title>
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap"
      rel="stylesheet"
    />
    <link href="./public/index.css" rel="stylesheet" />
    <link href="./public/media.css" rel="stylesheet" />
    <link href="./public/sidebars.css" rel="stylesheet" />
    <link href="./public/navbar.css" rel="stylesheet" />
    <script src="https://code.jquery.com/jquery-3.3.1.min.js"></script>
    <script src="./public/js/base.js"></script>
  </head>

  <body>
  <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://liang-zx.github.io/">
      <span class="icon">
          <svg class="svg-inline--fa fa-home fa-w-18" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="home" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512" data-fa-i2svg=""><path fill="currentColor" d="M280.37 148.26L96 300.11V464a16 16 0 0 0 16 16l112.06-.29a16 16 0 0 0 15.92-16V368a16 16 0 0 1 16-16h64a16 16 0 0 1 16 16v95.64a16 16 0 0 0 16 16.05L464 480a16 16 0 0 0 16-16V300L295.67 148.26a12.19 12.19 0 0 0-15.3 0zM571.6 251.47L488 182.56V44.05a12 12 0 0 0-12-12h-56a12 12 0 0 0-12 12v72.61L318.47 43a48 48 0 0 0-61 0L4.34 251.47a12 12 0 0 0-1.6 16.9l25.5 31A12 12 0 0 0 45.15 301l235.22-193.74a12.19 12.19 0 0 1 15.3 0L530.9 301a12 12 0 0 0 16.9-1.6l25.5-31a12 12 0 0 0-1.7-16.93z"></path></svg>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          Related Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://adaptdiffuser.github.io/">
            AdaptDiffuser
          </a>
          <a class="navbar-item" href="https://metadiffuser.github.io/">
            MetaDiffuser
          </a>
          <a class="navbar-item" href="https://skilldiffuser.github.io/">
            SkillDiffuser
          </a>
          <a class="navbar-item" href="https://dexdiffuser.github.io/">
            DexHandDiff
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>

    <div class="sidebarsWrapper">
      <div class="sidebars">
        <a class="barWrapper" clear href="#abstract-a" id="bar2"
          ><span>Abstract</span>
          <div class="bar"></div
        ></a>
        <a class="barWrapper" clear href="#tr2-a" id="bar3"
          ><span>Framework of AdaptDiffuser</span>
          <div class="bar"></div
        ></a>
        <a class="barWrapper" clear href="#results-a" id="bar4"
          ><span>Results</span>
          <div class="bar"></div
        ></a>
<!--        <a class="barWrapper" clear href="#attn-a" id="bar5"-->
<!--          ><span>Attention Analysis</span>-->
<!--          <div class="bar"></div-->
<!--        ></a>-->
      </div>
    </div>
    <main class="content">
      <section class="heading" style="text-align: center!important;">
        <h1 class="title">
          AdaptDiffuser: Diffusion Models as Adaptive Self-evolving Planners
        </h1>
        <section class="authors">
          <ul>
            <li>
              <span
                ><a
                  href="https://liang-zx.github.io/"
                  rel="noreferrer"
                  target="_blank"
              >Zhixuan Liang</a
                ><sup>1</sup></span
              >
            </li>
            <li>
              <span
                ><a
                  href="https://yaomarkmu.github.io/"
                  rel="noreferrer"
                  target="_blank"
                  >Yao Mu</a
                ><sup>1</sup></span
              >
            </li>
            <li>
              <span
                ><a
                  href="https://dingmyu.github.io/"
                  rel="noreferrer"
                  target="_blank"
                  >Mingyu Ding</a
                ><sup>1 2</sup></span
              >
            </li>
            <li>
              <span
                ><a
                  href="https://fei-ni.github.io/"
                  rel="noreferrer"
                  target="_blank"
                  >Fei Ni</a
                ><sup>3</sup></span
              >
            </li>
            <li>
              <span
                ><a
                  href="https://me.berkeley.edu/people/masayoshi-tomizuka/"
                  rel="noreferrer"
                  target="_blank"
                  >Masayoshi Tomizuka</a
                ><sup>2</sup></span
              >
            </li>
            <li>
              <span
                ><a
                  href="http://luoping.me/"
                  rel="noreferrer"
                  target="_blank"
                  >Ping Luo</a
                ><sup>1 4</sup></span
              >
            </li>
          </ul>
        </section>
        <section class="affiliations">
          <ul>
            <li><sup>1</sup>The University of Hong Kong,</li>
            <li><sup>2</sup>UC Berkeley,</li>
            <li><sup>3</sup>Tianjin University,</li>
            <li><sup>4</sup>Shanghai AI Laboratory</li>
          </ul>
        </section>
        <section class="conference">
          <h3>
          ICML 2023 Oral
          </h3>
        </section>
        <section class="links">
          <ul>
            <a href="https://arxiv.org/abs/2302.01877" rel="noreferrer" target="_blank">
              <li>
                <span class="icon"> <img src="./public/paper.svg" /> </span
                ><span>Paper</span>
              </li>
            </a>
<!--            <a-->
<!--              href="https://youtu.be/ZjRYlRRV9IA"-->
<!--              rel="noreferrer"-->
<!--              target="_blank"-->
<!--            >-->
<!--              <li>-->
<!--                <span class="icon"> <img src="./public/video.svg" /> </span-->
<!--                ><span>Video</span>-->
<!--              </li>-->
<!--            </a>-->
            <a
              href="https://github.com/Liang-ZX/adaptdiffuser"
              rel="noreferrer"
              target="_blank"
            >
              <li>
                <span class="icon">
                  <img src="./public/github.svg" />
                </span>
                <span>Code</span>
              </li>
            </a>
            <a
              href="https://icml.cc/virtual/2023/oral/25574"
              rel="noreferrer"
              target="_blank"
             >
               <li><span class="icon"> <img src="./public/video.svg"/> </span
                ><span>Oral Talk</span>
               </li>
             </a>
            <a
              href="https://icml.cc/media/PosterPDFs/ICML%202023/25146.png?t=1689061703.9673672"
              rel="noreferrer"
              target="_blank"
            >
              <li>
                <span class="icon"> <img src="./public/poster.svg" style="width: 120%;filter: invert(100%); stroke-width: 200%"/> </span
                ><span>Poster</span>
              </li>
            </a>
            <!-- <a><li>Video</li></a> -->
          </ul>
        </section>
        <a class="anchor" id="abstract-a"></a>
        <h2>Abstract</h2>
        <p class="abstract" style="font-family: 'Times New Roman', Arial; text-align: justify">
          Diffusion models have demonstrated their powerful generative capability in many tasks, with great potential to serve as a paradigm for offline reinforcement learning. However, the quality of the diffusion model is limited by the insufficient diversity of training data, which hinders the performance of planning and the generalizability to new tasks. This paper introduces AdaptDiffuser, an evolutionary planning method with diffusion that can self-evolve to improve the diffusion model hence a better planner, not only for seen tasks but can also adapt to unseen tasks. AdaptDiffuser enables the generation of rich synthetic expert data for goal-conditioned tasks using guidance from reward gradients. It then selects high-quality data via a discriminator to finetune the diffusion model, which improves the generalization ability to unseen tasks. Empirical experiments on two benchmark environments and two carefully designed unseen tasks in KUKA industrial robot arm and Maze2D environments demonstrate the effectiveness of AdaptDiffuser. For example, AdaptDiffuser not only outperforms the previous art <a href="https://diffusion-planning.github.io/" rel="noreferrer" target="_blank">Diffuser</a> by 20.8% on Maze2D and 7.5% on MuJoCo locomotion, but also adapts better to new tasks, e.g., KUKA pick-and-place, by 27.9% without requiring additional expert data.
        </p>
      </section>
      <section class="head-media">
        <div style="display: flex; width: 100%; height:auto; margin: auto; justify-content: center; align-items: center">
          <img
          style="width: 50%; height: auto"
          src="./public/images/bar.png"
          />
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <p class="caption" style="text-align: justify; font-family: 'Times New Roman'; width: 40%">
            <strong>Overall framework and performance comparison of AdaptDiffuser.</strong> It enables diffusion models to generate rich synthetic expert data using guidance from reward gradients of either seen or unseen tasks. Then, it iteratively selects high-quality data via a discriminator to fine-tune the model for self-evolving, leading to improved performance on seen tasks and better generalizability to unseen tasks.
          </p>
        </div>
      </section>

      <a class="anchor" id="tr2-a"></a>
      <section class="details">
        <h2>Framework of AdaptDiffuser</h2>
        <p style="font-family: 'Times New Roman',serif"> To improve the adaptability of the diffusion model to diverse tasks, rich data with distinct objectives is generated, guided by each task’s reward function. During the diffusion denoising process, we utilize a pre-trained denoising U-Net to progressively generate high-quality trajectories. At each denoising time step, we take the task-specific reward of a trajectory to adjust the gradient of state and action sequence, thereby creating trajectories that align with specific task objectives. Subsequently, the generated synthetic trajectory is evaluated by a discriminator to see if it meets the standards. If yes, it is incorporated into a data pool to fine-tune the diffusion model. The procedure iteratively enhances the generalizability of our model for both seen and unseen settings.
        </p>

        <div style="display: flex; margin: auto; width: 95%; height: auto">
          <img
          style="width: 50%; height: auto"
          src="./public/images/framework.png"
          />
          &nbsp;&nbsp;&nbsp;&nbsp;
          <br />
          <img
          style="width: 50%; height: auto"
          src="./public/video/video1.gif"
          />
        </div>
        <br />

        <a class="anchor" id="results-a"></a>
        <h2>Results</h2>
        <h3 style="margin-bottom: 0">Maze2D Navigation Task</h3>
        <div style="display: flex; margin: auto; width: 95%; height: auto; justify-content: center">
        <img
          style="width: 65%; height: auto"
          src="./public/images/maze1.png"
        />
        </div>
        <p class="caption" style="text-align: justify; font-family: 'Times New Roman',serif; margin-top: 0; margin-bottom: 0">
          <strong>Hard Cases of Maze2D with Long Planning Path.</strong> Paths are generated in the Maze2D with a specified start and goal condition.
        </p>
        <div style="display: flex; margin: auto; width: 95%; height: auto; justify-content: center">
        <img
          style="width: 65%; height: auto"
          src="./public/images/maze_gold.png"
        />
        </div>
        <p class="caption" style="text-align: justify; font-family: 'Times New Roman',serif; margin-top: 0">
          <strong>Maze2d Navigation with Gold Coin Picking Task.</strong> Subfigures (a) (b) show the optimal path when there are no gold coins in the Maze. (The generated routes walk at the bottom of the Maze.) And subfigures (c) (d) add additional reward in (4, 2) position of the Maze. The planners generate new paths that pass through the gold coin as shown in subfigures (c) (d). (The newly generated routes walk in the middle of the maze.)
        </p>
        <h3 style="margin-bottom: 0">MuJoCo Locomotion Task</h3>
        <div style="display: flex; margin: auto; width: 95%; height: auto; justify-content: space-between">
          <video autoplay="" muted="" loop="" width="25%">
          <source src="./public/video/fine/halfcheetah-medium.mp4" type="video/mp4" />
        </video>

        <video autoplay="" muted="" loop="" width="25%">
          <source src="./public/video/fine/hopper-medium-v2.mp4" type="video/mp4" />
        </video>

          <video autoplay="" muted="" loop="" width="25%">
          <source src="./public/video/fine/walker2d-medium-v2.mp4" type="video/mp4" />
        </video>
        </div>
        <div class="col-title" style="display: flex; width: 95%; height: auto; justify-content: space-between; margin: 0; font-family: 'Times New Roman',serif">
            <p style="margin-left: 4em">HalfCheetah Medium</p>
            <p>Hopper Medium</p>
            <p style="margin-right: 2em">Walker2d Medium</p>
          </div>
        <h3>Demos of KUKA Pick and Place Task</h3>
        <div style="display: flex; margin: auto; width: 95%; height: auto">
        <video autoplay="" muted="" loop="" width="50%">
          <source src="./public/video/video2.mp4" type="video/mp4" />
        </video>
          &nbsp;&nbsp;&nbsp;&nbsp;
          <video autoplay="" muted="" loop="" width="50%">
          <source src="./public/video/video1.mp4" type="video/mp4" />
        </video>
        </div>

<!--        <a class="anchor" id="attn-a"></a>-->
<!--        <h2>Attention Analysis</h2>-->
<!--        <p>-->
<!--          To get an insight into how the transformer architecture enables the-->
<!--          policy to solve environments more succesfully, we analyze the learned-->
<!--          attention on the Couch Moving environment.-->
<!--        </p>-->
<!--        <div class="attn-video" style="text-align: center">-->
<!--          <video autoplay="" muted="" loop="" height="100%" style="width: 75%">-->
<!--            <source src="./public/videos/attn.mp4" type="video/mp4" />-->
<!--          </video>-->
<!--        </div>-->
      </section>
      <section class="citation">
        <h2>Bibtex</h2>
        <pre>
<code>@inproceedings{liang2023adaptdiffuser,
  title={AdaptDiffuser: Diffusion Models as Adaptive Self-evolving Planners},
  author={Liang, Zhixuan and Mu, Yao and Ding, Mingyu and Ni, Fei and Tomizuka, Masayoshi and Luo, Ping},
  booktitle={International Conference on Machine Learning},
  pages={20725--20745},
  year={2023},
  organization={PMLR}
}</code></pre>
      </section>
      <br />
<!--      <section class="acknowledgements">-->
<!--        <h2>Acknowledgements</h2>-->
<!--        <p style="font-family: 'Times New Roman', Arial;">-->
<!--          This paper is partially supported by the National Key R&D Program of China No.2022ZD0161000 and the General Research Fund of Hong Kong No.17200622. Special thanks to additional members of the HKU-MMLab for writing feedback.-->
<!--        </p>-->
<!--      </section>-->
    </main>
  </body>
</html>
